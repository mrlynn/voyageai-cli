/**
 * Document Ingestion Pipeline
 * Generated by vai v{{vaiVersion}} on {{generatedAt}}
 * 
 * Model: {{model}}
 * Chunk Strategy: {{chunkStrategy}}
 * Chunk Size: {{chunkSize}} characters
 * Overlap: {{chunkOverlap}} characters
 */

const fs = require('fs');
const path = require('path');
const { embed } = require('./client');
const { insertDocuments, close } = require('./connection');

/**
 * Chunk text using {{chunkStrategy}} strategy.
 * @param {string} text - Text to chunk
 * @param {object} options - Chunking options
 * @returns {string[]} Array of chunks
 */
function chunkText(text, options = {}) {
  const size = options.size || {{chunkSize}};
  const overlap = options.overlap || {{chunkOverlap}};
  
{{#if chunkStrategy}}
{{#unless chunkStrategy}}
  // Fixed-size chunking
  const chunks = [];
  let start = 0;
  
  while (start < text.length) {
    const end = Math.min(start + size, text.length);
    chunks.push(text.slice(start, end).trim());
    start = end - overlap;
    if (start >= text.length) break;
  }
  
  return chunks.filter(c => c.length > 0);
{{/unless}}
{{/if}}
  // Recursive chunking with smart boundaries
  const separators = ['\n\n', '\n', '. ', ' '];
  
  function splitRecursive(text, separatorIndex = 0) {
    if (text.length <= size) {
      return [text.trim()].filter(c => c.length > 0);
    }
    
    if (separatorIndex >= separators.length) {
      // Fall back to fixed-size split
      const chunks = [];
      let start = 0;
      while (start < text.length) {
        chunks.push(text.slice(start, start + size).trim());
        start += size - overlap;
      }
      return chunks.filter(c => c.length > 0);
    }
    
    const separator = separators[separatorIndex];
    const parts = text.split(separator);
    const chunks = [];
    let current = '';
    
    for (const part of parts) {
      const potential = current ? current + separator + part : part;
      
      if (potential.length <= size) {
        current = potential;
      } else {
        if (current) chunks.push(...splitRecursive(current, separatorIndex + 1));
        current = part;
      }
    }
    
    if (current) {
      chunks.push(...splitRecursive(current, separatorIndex + 1));
    }
    
    return chunks;
  }
  
  return splitRecursive(text);
}

/**
 * Read and parse a file based on extension.
 * @param {string} filePath - Path to the file
 * @returns {{text: string, metadata: object}}
 */
function readFile(filePath) {
  const ext = path.extname(filePath).toLowerCase();
  const content = fs.readFileSync(filePath, 'utf8');
  
  const metadata = {
    source: filePath,
    filename: path.basename(filePath),
    extension: ext,
  };
  
  // For markdown, optionally extract frontmatter
  if (ext === '.md' || ext === '.mdx') {
    const frontmatterMatch = content.match(/^---\n([\s\S]*?)\n---\n([\s\S]*)$/);
    if (frontmatterMatch) {
      // Simple YAML-like parsing for common fields
      const yaml = frontmatterMatch[1];
      const body = frontmatterMatch[2];
      
      const titleMatch = yaml.match(/title:\s*["']?(.+?)["']?\s*$/m);
      if (titleMatch) metadata.title = titleMatch[1];
      
      return { text: body, metadata };
    }
  }
  
  return { text: content, metadata };
}

/**
 * Ingest a single file: read, chunk, embed, and store.
 * @param {string} filePath - Path to the file
 * @param {object} options - Ingestion options
 * @returns {Promise<{chunks: number, tokens: number}>}
 */
async function ingestFile(filePath, options = {}) {
  const { text, metadata } = readFile(filePath);
  const chunks = chunkText(text, options);
  
  if (chunks.length === 0) {
    return { chunks: 0, tokens: 0 };
  }
  
  // Embed all chunks
  const { embeddings, usage } = await embed(chunks, { inputType: 'document' });
  
  // Prepare documents for insertion
  const documents = chunks.map((chunk, i) => ({
    text: chunk,
    embedding: embeddings[i],
    metadata: {
      ...metadata,
      chunkIndex: i,
      totalChunks: chunks.length,
    },
  }));
  
  // Insert into MongoDB
  await insertDocuments(documents);
  
  return {
    chunks: chunks.length,
    tokens: usage.total_tokens,
  };
}

/**
 * Ingest multiple files from a directory.
 * @param {string} dirPath - Directory path
 * @param {object} options - Ingestion options
 * @param {string[]} options.extensions - File extensions to include (default: ['.txt', '.md'])
 * @returns {Promise<{files: number, chunks: number, tokens: number}>}
 */
async function ingestDirectory(dirPath, options = {}) {
  const extensions = options.extensions || ['.txt', '.md', '.mdx'];
  
  function findFiles(dir) {
    const files = [];
    const entries = fs.readdirSync(dir, { withFileTypes: true });
    
    for (const entry of entries) {
      const fullPath = path.join(dir, entry.name);
      if (entry.isDirectory() && !entry.name.startsWith('.')) {
        files.push(...findFiles(fullPath));
      } else if (entry.isFile() && extensions.includes(path.extname(entry.name).toLowerCase())) {
        files.push(fullPath);
      }
    }
    
    return files;
  }
  
  const files = findFiles(dirPath);
  let totalChunks = 0;
  let totalTokens = 0;
  
  for (const file of files) {
    console.log(`Ingesting: ${file}`);
    const { chunks, tokens } = await ingestFile(file, options);
    totalChunks += chunks;
    totalTokens += tokens;
  }
  
  return {
    files: files.length,
    chunks: totalChunks,
    tokens: totalTokens,
  };
}

// CLI entry point
if (require.main === module) {
  const args = process.argv.slice(2);
  const target = args[0] || '.';
  
  (async () => {
    try {
      const stats = fs.statSync(target);
      
      if (stats.isDirectory()) {
        const result = await ingestDirectory(target);
        console.log(`\n✓ Ingested ${result.files} files (${result.chunks} chunks, ${result.tokens} tokens)`);
      } else {
        const result = await ingestFile(target);
        console.log(`\n✓ Ingested ${result.chunks} chunks (${result.tokens} tokens)`);
      }
      
      await close();
    } catch (err) {
      console.error('Error:', err.message);
      process.exit(1);
    }
  })();
}

module.exports = {
  chunkText,
  readFile,
  ingestFile,
  ingestDirectory,
};
